{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Workshop: Building an Information Retrieval System for Podcast Episodes\n",
    "\n",
    "## Objective:\n",
    "Create an Information Retrieval (IR) system that processes a dataset of podcast transcripts and, given a query, returns the episodes where the host and guest discuss the query topic. Use TF-IDF and BERT for vector space representation and compare the results."
   ],
   "id": "333ae546d607a744"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Instructions:\n",
    "\n",
    "### Step 1: Import Libraries\n",
    "Import necessary libraries for data handling, text processing, and machine learning.\n",
    "\n",
    "### Step 2: Load the Dataset\n",
    "\n",
    "Load the dataset of podcast transcripts.\n",
    "\n",
    "Find the dataset in: https://www.kaggle.com/datasets/rajneesh231/lex-fridman-podcast-transcript\n",
    "\n",
    "### Step 3: Text Preprocessing\n",
    "\n",
    "You know what to do ;)\n",
    "\n",
    "###  Step 4: Vector Space Representation - TF-IDF\n",
    "\n",
    "Create TF-IDF vector representations of the transcripts.\n",
    "\n",
    "### Step 5: Vector Space Representation - BERT\n",
    "\n",
    "Create BERT vector representations of the transcripts using a pre-trained BERT model.\n",
    "\n",
    "### Step 6: Query Processing\n",
    "\n",
    "Define a function to process the query and compute similarity scores using both TF-IDF and BERT embeddings.\n",
    "\n",
    "### Step 7: Retrieve and Compare Results\n",
    "\n",
    "Define a function to retrieve the top results based on similarity scores for both TF-IDF and BERT representations.\n",
    "\n",
    "### Step 8: Test the IR System\n",
    "\n",
    "Test the system with a sample query.\n",
    "\n",
    "Retrieve and display the top results using both TF-IDF and BERT representations.\n",
    "\n",
    "### Step 9: Compare Results\n",
    "\n",
    "Analyze and compare the results obtained from TF-IDF and BERT representations.\n",
    "\n",
    "Discuss the differences, strengths, and weaknesses of each method based on the retrieval results.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "* Follow the steps outlined above to implement the IR system.\n",
    "* Run the provided code snippets to understand how each part of the system works.\n",
    "* Test the system with various queries to observe the results from both TF-IDF and BERT representations.\n",
    "* Compare and analyze the results. Discuss the pros and cons of each method.\n",
    "* Document your findings and any improvements you make to the system."
   ],
   "id": "6a934919d95ac2de"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import string"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Load the Dataset\n",
    "\n",
    "Load the dataset of podcast transcripts."
   ],
   "id": "91a2faa7e20eb347"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/podcastdata_dataset.csv')#, index_col=0)\n",
    "print(df.head())"
   ],
   "id": "1e8f07a38e5d7d99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T14:01:02.215517Z",
     "start_time": "2024-07-04T14:01:02.203346Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.shape)",
   "id": "a72b29b90248e314",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319, 6)\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Text Preprocessing\n",
    "* Delete punctuation\n",
    "* Delete stop words"
   ],
   "id": "27d2bdbcfe1c15b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corpus = df['text']\n",
    "print(corpus.head())"
   ],
   "id": "96e27a436acfee0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First, we delete punctuation\n",
    "corpus_nopunct = []\n",
    "for doc in corpus:\n",
    "    corpus_nopunct.append(doc.lower().translate(str.maketrans('', '', string.punctuation)))"
   ],
   "id": "347f2257b3efb299",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(corpus_nopunct[:10])",
   "id": "40b27808818cb1d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['text_nopunct'] = corpus_nopunct\n",
    "print(df.head())"
   ],
   "id": "c317de8840b34383",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stopw = set(stopwords.words('english'))"
   ],
   "id": "c88ebb046eec818b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(stopw))",
   "id": "63a479cb62f8edc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corpus_nostopw = []\n",
    "# TODO: This code should be optimized\n",
    "for doc in corpus_nopunct:\n",
    "    clean_doc = []\n",
    "    doc_array = doc.split(' ')\n",
    "    for word in doc_array:\n",
    "        if word not in stopw:\n",
    "            clean_doc.append(word)\n",
    "    corpus_nostopw.append(' '.join(clean_doc))"
   ],
   "id": "cce2de0e89afbf2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "corpus_nostopw[300]",
   "id": "ec421407d1f4dade",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['text_nostopw'] = corpus_nostopw\n",
    "print(df.head())"
   ],
   "id": "a21c3c4d767cc454",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Step 4: Vector Space Representation - TF-IDF\n",
    "\n",
    "Create TF-IDF vector representations of the transcripts."
   ],
   "id": "20c778562851705c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_mtx = vectorizer.fit_transform(df['text_nostopw'])"
   ],
   "id": "194bfa099f153563",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "query = 'Computer Science' ",
   "id": "6728d0e8e7a46947",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "query_vector = vectorizer.transform([query])",
   "id": "43a9c199243587af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(tfidf_mtx, query_vector)"
   ],
   "id": "cc69c6d91a1cb1de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(similarities)",
   "id": "2eca9305b19cd8ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "6af8890a1284ac54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "similarities_df = pd.DataFrame(similarities, columns=['sim'])\n",
    "similarities_df['ep'] = df['title']\n",
    "print(similarities_df.head())"
   ],
   "id": "ed221e850ec1310",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "similarities_df",
   "id": "81574979e472f363",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5: Vector Space Representation - BERT\n",
    "\n",
    "Create BERT vector representations of the transcripts using a pre-trained BERT model."
   ],
   "id": "be2e67c2290efbd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')"
   ],
   "id": "4db1a92bbeb90308",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:44:27.770883Z",
     "start_time": "2024-07-04T13:43:41.292822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def generate_bert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "    return np.array(embeddings).transpose(0,2,1)\n",
    "\n",
    "corpus_bert = generate_bert_embeddings(corpus[:50])"
   ],
   "id": "d1266c8c3ebdfab4",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:46:01.011532Z",
     "start_time": "2024-07-04T13:46:01.000558Z"
    }
   },
   "cell_type": "code",
   "source": "corpus_bert.shape",
   "id": "33e9108e0cda331d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 768, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:51:03.861203Z",
     "start_time": "2024-07-04T13:51:03.685627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = ['Computer Science']\n",
    "query_bert = generate_bert_embeddings(query)"
   ],
   "id": "18f31673a9f21ed5",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:51:04.329074Z",
     "start_time": "2024-07-04T13:51:04.321648Z"
    }
   },
   "cell_type": "code",
   "source": "query_bert.shape",
   "id": "8524fd0f5c453318",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768, 1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:51:18.635857Z",
     "start_time": "2024-07-04T13:51:18.614413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similarities = cosine_similarity(corpus_bert.reshape(50,768), query_bert.reshape(1,768))\n",
    "\n",
    "# similarities_bert = cosine_similarity(query_bert.reshape(1, -1), corpus_bert.squeeze())\n",
    "similarities"
   ],
   "id": "28fe28b9e22a3cdc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64740765],\n",
       "       [0.65648293],\n",
       "       [0.62955046],\n",
       "       [0.5798758 ],\n",
       "       [0.6637971 ],\n",
       "       [0.68807083],\n",
       "       [0.652893  ],\n",
       "       [0.5962678 ],\n",
       "       [0.6157255 ],\n",
       "       [0.6364522 ],\n",
       "       [0.6226719 ],\n",
       "       [0.65230983],\n",
       "       [0.6951232 ],\n",
       "       [0.6479816 ],\n",
       "       [0.6479816 ],\n",
       "       [0.69003546],\n",
       "       [0.67366695],\n",
       "       [0.6663608 ],\n",
       "       [0.5504489 ],\n",
       "       [0.6264469 ],\n",
       "       [0.69075406],\n",
       "       [0.5804626 ],\n",
       "       [0.62647045],\n",
       "       [0.6380265 ],\n",
       "       [0.59840155],\n",
       "       [0.66201544],\n",
       "       [0.65600586],\n",
       "       [0.60880053],\n",
       "       [0.61693186],\n",
       "       [0.6207942 ],\n",
       "       [0.6466563 ],\n",
       "       [0.66204685],\n",
       "       [0.66466063],\n",
       "       [0.69745606],\n",
       "       [0.70639443],\n",
       "       [0.6530784 ],\n",
       "       [0.60017955],\n",
       "       [0.662514  ],\n",
       "       [0.6738858 ],\n",
       "       [0.6957747 ],\n",
       "       [0.65353596],\n",
       "       [0.6237168 ],\n",
       "       [0.632741  ],\n",
       "       [0.65763825],\n",
       "       [0.68119484],\n",
       "       [0.64490247],\n",
       "       [0.61485004],\n",
       "       [0.5823724 ],\n",
       "       [0.67291915],\n",
       "       [0.722841  ]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:51:56.069446Z",
     "start_time": "2024-07-04T13:51:56.053251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_bert(query):\n",
    "    query_bert = generate_bert_embeddings(query)\n",
    "    similarities = cosine_similarity(corpus_bert.reshape(50,768), query_bert.reshape(1,768))\n",
    "    similarities_df = pd.DataFrame(similarities, columns=['sim'])\n",
    "    similarities_df['ep'] = df['title']\n",
    "    return similarities_df"
   ],
   "id": "4d808c3466fb39dc",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:58:56.309466Z",
     "start_time": "2024-07-04T13:58:56.104624Z"
    }
   },
   "cell_type": "code",
   "source": "retrieve_bert(['gpt'])",
   "id": "682c5000551cbd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         sim  \\\n",
       "0   0.608030   \n",
       "1   0.606848   \n",
       "2   0.568143   \n",
       "3   0.528032   \n",
       "4   0.615966   \n",
       "5   0.631584   \n",
       "6   0.601826   \n",
       "7   0.546332   \n",
       "8   0.549860   \n",
       "9   0.583512   \n",
       "10  0.571838   \n",
       "11  0.605585   \n",
       "12  0.637021   \n",
       "13  0.598721   \n",
       "14  0.598721   \n",
       "15  0.615886   \n",
       "16  0.610613   \n",
       "17  0.618922   \n",
       "18  0.554204   \n",
       "19  0.593378   \n",
       "20  0.646843   \n",
       "21  0.547627   \n",
       "22  0.614297   \n",
       "23  0.597502   \n",
       "24  0.541601   \n",
       "25  0.634185   \n",
       "26  0.637171   \n",
       "27  0.573322   \n",
       "28  0.591670   \n",
       "29  0.585411   \n",
       "30  0.620383   \n",
       "31  0.620759   \n",
       "32  0.609695   \n",
       "33  0.647951   \n",
       "34  0.654667   \n",
       "35  0.603648   \n",
       "36  0.547104   \n",
       "37  0.592781   \n",
       "38  0.640236   \n",
       "39  0.660287   \n",
       "40  0.595069   \n",
       "41  0.582677   \n",
       "42  0.578582   \n",
       "43  0.630989   \n",
       "44  0.639756   \n",
       "45  0.610604   \n",
       "46  0.582491   \n",
       "47  0.542377   \n",
       "48  0.624068   \n",
       "49  0.703856   \n",
       "\n",
       "                                                                            ep  \n",
       "0                                                                     Life 3.0  \n",
       "1                                                                Consciousness  \n",
       "2                                                      AI in the Age of Reason  \n",
       "3                                                                Deep Learning  \n",
       "4                                                         Statistical Learning  \n",
       "5                                                                       Python  \n",
       "6                                             Stack Overflow and Coding Horror  \n",
       "7                                                                       Google  \n",
       "8                                                       Long-Term Future of AI  \n",
       "9                                                  Deep Reinforcement Learning  \n",
       "10                                    Godel Machines, Meta-Learning, and LSTMs  \n",
       "11                                                       Poker and Game Theory  \n",
       "12                                                 Brains, Minds, and Machines  \n",
       "13                                                           Cruise Automation  \n",
       "14                                          Ask Me Anything – AMA January 2021  \n",
       "15                              Reinforcement Learning, Planning, and Robotics  \n",
       "16                           Revolutionary Ideas in Science, Math, and Society  \n",
       "17                                                              OpenAI and AGI  \n",
       "18                                                             Tesla Autopilot  \n",
       "19                                      Generative Adversarial Networks (GANs)  \n",
       "20                      DeepMind AlphaStar, StarCraft, Language, and Sequences  \n",
       "21                            Compilers, LLVM, Swift, TPU, and ML Accelerators  \n",
       "22                                                                  TensorFlow  \n",
       "23                                                              Adobe Research  \n",
       "24                           Affective Computing, Emotion, Privacy, and Health  \n",
       "25                                      Thousand Brains Theory of Intelligence  \n",
       "26                          The Nature of the Universe, Life, and Intelligence  \n",
       "27                                   AI Superpowers – China and Silicon Valley  \n",
       "28                         Self-Driving Cars at Aurora, Google, CMU, and DARPA  \n",
       "29                                                                     Spotify  \n",
       "30                                                               Microsoft CTO  \n",
       "31                                Comma.ai, OpenPilot, and Autonomous Vehicles  \n",
       "32                                Brain Development from Stem Cell to Organoid  \n",
       "33                                                             Lockheed Martin  \n",
       "34                                 Machines Who Think and the Early Days of AI  \n",
       "35                                  fast.ai Deep Learning Courses and Research  \n",
       "36  Deep Learning, Convolutional Neural Networks, and Self-Supervised Learning  \n",
       "37                                                               Flying Robots  \n",
       "38                                Keras, Deep Learning, and the Progress of AI  \n",
       "39                                                                      iRobot  \n",
       "40                            Deep Learning for Cancer Diagnosis and Treatment  \n",
       "41                           Quantum Mechanics, String Theory, and Black Holes  \n",
       "42                                  Artificial Intelligence: A Modern Approach  \n",
       "43                            Toward a Hybrid of Deep Learning and Symbolic AI  \n",
       "44                           IBM Watson, Jeopardy & Deep Conversations with AI  \n",
       "45                            Future of Humans, Aliens, Space Travel & Physics  \n",
       "46                                             Chess, Deep Blue, AI, and Putin  \n",
       "47                        Quantum Mechanics and the Many-Worlds Interpretation  \n",
       "48                                                                         C++  \n",
       "49                             Neuralink, AI, Autopilot, and the Pale Blue Dot  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>ep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.608030</td>\n",
       "      <td>Life 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.606848</td>\n",
       "      <td>Consciousness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.568143</td>\n",
       "      <td>AI in the Age of Reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528032</td>\n",
       "      <td>Deep Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.615966</td>\n",
       "      <td>Statistical Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.631584</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.601826</td>\n",
       "      <td>Stack Overflow and Coding Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.546332</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.549860</td>\n",
       "      <td>Long-Term Future of AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.583512</td>\n",
       "      <td>Deep Reinforcement Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.571838</td>\n",
       "      <td>Godel Machines, Meta-Learning, and LSTMs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.605585</td>\n",
       "      <td>Poker and Game Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.637021</td>\n",
       "      <td>Brains, Minds, and Machines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.598721</td>\n",
       "      <td>Cruise Automation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.598721</td>\n",
       "      <td>Ask Me Anything – AMA January 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615886</td>\n",
       "      <td>Reinforcement Learning, Planning, and Robotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.610613</td>\n",
       "      <td>Revolutionary Ideas in Science, Math, and Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.618922</td>\n",
       "      <td>OpenAI and AGI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.554204</td>\n",
       "      <td>Tesla Autopilot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.593378</td>\n",
       "      <td>Generative Adversarial Networks (GANs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.646843</td>\n",
       "      <td>DeepMind AlphaStar, StarCraft, Language, and Sequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.547627</td>\n",
       "      <td>Compilers, LLVM, Swift, TPU, and ML Accelerators</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.614297</td>\n",
       "      <td>TensorFlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.597502</td>\n",
       "      <td>Adobe Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.541601</td>\n",
       "      <td>Affective Computing, Emotion, Privacy, and Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.634185</td>\n",
       "      <td>Thousand Brains Theory of Intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.637171</td>\n",
       "      <td>The Nature of the Universe, Life, and Intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.573322</td>\n",
       "      <td>AI Superpowers – China and Silicon Valley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.591670</td>\n",
       "      <td>Self-Driving Cars at Aurora, Google, CMU, and DARPA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.585411</td>\n",
       "      <td>Spotify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.620383</td>\n",
       "      <td>Microsoft CTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.620759</td>\n",
       "      <td>Comma.ai, OpenPilot, and Autonomous Vehicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.609695</td>\n",
       "      <td>Brain Development from Stem Cell to Organoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.647951</td>\n",
       "      <td>Lockheed Martin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.654667</td>\n",
       "      <td>Machines Who Think and the Early Days of AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.603648</td>\n",
       "      <td>fast.ai Deep Learning Courses and Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.547104</td>\n",
       "      <td>Deep Learning, Convolutional Neural Networks, and Self-Supervised Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.592781</td>\n",
       "      <td>Flying Robots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.640236</td>\n",
       "      <td>Keras, Deep Learning, and the Progress of AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.660287</td>\n",
       "      <td>iRobot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.595069</td>\n",
       "      <td>Deep Learning for Cancer Diagnosis and Treatment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.582677</td>\n",
       "      <td>Quantum Mechanics, String Theory, and Black Holes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.578582</td>\n",
       "      <td>Artificial Intelligence: A Modern Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.630989</td>\n",
       "      <td>Toward a Hybrid of Deep Learning and Symbolic AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.639756</td>\n",
       "      <td>IBM Watson, Jeopardy &amp; Deep Conversations with AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.610604</td>\n",
       "      <td>Future of Humans, Aliens, Space Travel &amp; Physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.582491</td>\n",
       "      <td>Chess, Deep Blue, AI, and Putin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.542377</td>\n",
       "      <td>Quantum Mechanics and the Many-Worlds Interpretation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.624068</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.703856</td>\n",
       "      <td>Neuralink, AI, Autopilot, and the Pale Blue Dot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 6: Query Processing\n",
    "\n",
    "Define a function to process the query and compute similarity scores using both TF-IDF and BERT embeddings."
   ],
   "id": "355046ba40ac6a7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:59:13.151983Z",
     "start_time": "2024-07-04T13:59:13.141079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_tfidf(query):\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(tfidf_mtx, query_vector)\n",
    "    similarities_df = pd.DataFrame(similarities, columns=['sim'])\n",
    "    similarities_df['ep'] = df['title']\n",
    "    return similarities_df"
   ],
   "id": "40fae62855ac9639",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:59:22.897410Z",
     "start_time": "2024-07-04T13:59:22.868743Z"
    }
   },
   "cell_type": "code",
   "source": "retrieve_tfidf('gpt')",
   "id": "fad41fd58707f266",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     sim                                                          ep\n",
       "0    0.0                                                    Life 3.0\n",
       "1    0.0                                               Consciousness\n",
       "2    0.0                                     AI in the Age of Reason\n",
       "3    0.0                                               Deep Learning\n",
       "4    0.0                                        Statistical Learning\n",
       "..   ...                                                         ...\n",
       "314  0.0             Singularity, Superintelligence, and Immortality\n",
       "315  0.0            Emotion AI, Social Robots, and Self-Driving Cars\n",
       "316  0.0   Comedy, MADtv, AI, Friendship, Madness, and Pro Wrestling\n",
       "317  0.0                                                       Poker\n",
       "318  0.0  Biology, Life, Aliens, Evolution, Embryogenesis & Xenobots\n",
       "\n",
       "[319 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>ep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Life 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Consciousness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>AI in the Age of Reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Deep Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Statistical Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Singularity, Superintelligence, and Immortality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Emotion AI, Social Robots, and Self-Driving Cars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Comedy, MADtv, AI, Friendship, Madness, and Pro Wrestling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Poker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Biology, Life, Aliens, Evolution, Embryogenesis &amp; Xenobots</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b1f8994f5ad615ad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
